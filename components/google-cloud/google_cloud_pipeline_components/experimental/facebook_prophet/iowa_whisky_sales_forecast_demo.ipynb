{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iowa Whisky Sales (prod)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZxBc_aRrXeA"
      },
      "source": [
        "# Time series forecasting with Facebook prophet & Vertex AI Managed Pipelines\n",
        "\n",
        "This notebook demonstrates how to train and deploy a Facebook Prophet time-series forecasting model using Vertex AI Managed Pipeline using components provided by Google.\n",
        "\n",
        "Models will be created to predict the sales of various types of whisky using the public Iowa Liquor Sales Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld36yEnnsBuE"
      },
      "source": [
        "## Configuration\n",
        "Configure the following values before proceeding with this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ode24E8tsLad"
      },
      "source": [
        "# A directory in GCS to save artifacts from pipeline components\n",
        "PIPELINE_ROOT = 'gs://...' # @param {type:'string'}\n",
        "# A GCP project to run the pipeline in\n",
        "GCP_PROJECT_ID = '' #@param {type:'string'}\n",
        "# A friendly name for the forecasting model(s) created in this pipeline \n",
        "VERTEX_MODEL_DISPLAY_NAME = 'iowa-whisky-sales-forecasting' #@param {type:'string'}\n",
        "# A directory in GCS to save the results of Vertex batch prediction\n",
        "BATCH_PREDICT_GCS_PREFIX = 'gs://.../prophet_batch_prediction/' #@param {type:'string'}\n",
        "# A directory in GCS to store model artifacts for Vertex batch prediction\n",
        "GCS_MODEL_ARTIFACT_DIR = 'gs://.../prophet_models/' #@param {type:'string'}\n",
        "# The path to a file in GCS that will be created by the pipeline to store the batch request payload\n",
        "BATCH_PREDICT_GCS_SOURCE = 'gs://.../batch-request.jsonl' #@param {type:'string'}\n",
        "# A friendly name for the batch request job\n",
        "BATCH_PREDICT_DISPLAY_NAME = 'batch-prediction' #@param {type:'string'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jXm2JUGu8Ce"
      },
      "source": [
        "Import and install the prerequisite python packages. **The notebook runtime must be restarted after installing packages**. Under the 'Runtime' tab above click 'Restart Runtime'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BeOIASpvDCz"
      },
      "source": [
        "%%capture\n",
        "!pip install google-cloud-core==1.7.1 kfp==1.6.4 google-cloud-aiplatform==1.1.1 google-cloud-storage==1.40.0 google_cloud_pipeline_components==0.1.3\n",
        "import json\n",
        "from typing import List\n",
        "\n",
        "from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
        "from google_cloud_pipeline_components.aiplatform import ModelUploadOp as VertexModelImportOp\n",
        "#from google_cloud_pipeline_components.google.facebook_prophet import FitProphetModelOp, ProphetPredictOp\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2 import dsl\n",
        "from kfp.v2.components import InputPath, OutputPath\n",
        "from kfp.v2.dsl import component\n",
        "from kfp.v2.dsl import ParallelFor\n",
        "from kfp.v2.google.client import AIPlatformClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwtG4haNvXHH"
      },
      "source": [
        "Authenticate and configure the Google Cloud SDK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlpElDZhvuUe"
      },
      "source": [
        "!gcloud config set project {GCP_PROJECT_ID}\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7aBOrSBStR-"
      },
      "source": [
        "# TODO: Remove this block when the components have been published.\n",
        "# Replace with python imports\n",
        "from kfp.v2.components import load_component_from_text\n",
        "import os\n",
        "import json\n",
        "\n",
        "_FitProphetModelOp = load_component_from_text(\"\"\"\n",
        "name: Facebook Prophet Model Fit\n",
        "description: Fit a time-series forecasting model with Facebook Prophet\n",
        "\n",
        "inputs:\n",
        "  - {name: Data Source, type: Dataset}\n",
        "  - {name: Features, type: List, optional: True}\n",
        "  - {name: Countries, type: List, optional: True}\n",
        "  - {name: N Changepoints, type: Integer, optional: True}\n",
        "  - {name: Changepoint Range, type: Float, optional: True}\n",
        "  - {name: Yearly Seasonality, type: String, optional: True}\n",
        "  - {name: Weekly Seasonality, type: String, optional: True}\n",
        "  - {name: Daily Seasonality, type: String, optional: True}\n",
        "  - {name: Seasonality Mode, type: String, optional: True}\n",
        "  - {name: Seasonality Prior Scale, type: Float, optional: True}\n",
        "  - {name: Holidays Prior Scale, type: Float, optional: True}\n",
        "  - {name: Changepoint Prior Scale, type: Float, optional: True}\n",
        "  - {name: Mcmc Samples, type: Integer, optional: True}\n",
        "  - {name: Interval Width, type: Float, optional: True}\n",
        "  - {name: Uncertainty Samples, type: Integer, optional: True}\n",
        "  - {name: Stan Backend, type: String, optional: True}\n",
        "  - {name: Cross Validation Horizon, type: String, optional: True}\n",
        "outputs:\n",
        "  - {name: Model, type: Model}\n",
        "  - {name: Cross Validation Results, type: Dataset}\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: us-docker.pkg.dev/vertex-ai/prophet/prophet\n",
        "    command: [\n",
        "      python3,\n",
        "      /pipelines/component/src/fit_model.py\n",
        "      ]\n",
        "    args:\n",
        "      - '--data_source'\n",
        "      - {inputPath: Data Source}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Features\n",
        "          then:\n",
        "            - '--features'\n",
        "            - {inputValue: Features}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Countries\n",
        "          then:\n",
        "            - '--countries'\n",
        "            - {inputValue: Countries}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: N Changepoints\n",
        "          then:\n",
        "            - '--n_changepoints'\n",
        "            - {inputValue: N Changepoints}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Changepoint Range\n",
        "          then:\n",
        "            - '--changepoint_range'\n",
        "            - {inputValue: Changepoint Range}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Yearly Seasonality\n",
        "          then:\n",
        "            - '--yearly_seasonality'\n",
        "            - {inputValue: Yearly Seasonality}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Weekly Seasonality\n",
        "          then:\n",
        "            - '--weekly_seasonality'\n",
        "            - {inputValue: Weekly Seasonality}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Daily Seasonality\n",
        "          then:\n",
        "            - '--daily_seasonality'\n",
        "            - {inputValue: Daily Seasonality}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Seasonality Mode\n",
        "          then:\n",
        "            - '--seasonality_mode'\n",
        "            - {inputValue: Seasonality Mode}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Seasonality Prior Scale\n",
        "          then:\n",
        "            - '--seasonality_prior_scale'\n",
        "            - {inputValue: Seasonality Prior Scale}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Holidays Prior Scale\n",
        "          then:\n",
        "            - '--holidays_prior_scale'\n",
        "            - {inputValue: Holidays Prior Scale}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Changepoint Prior Scale\n",
        "          then:\n",
        "            - '--changepoint_prior_scale'\n",
        "            - {inputValue: Changepoint Prior Scale}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Mcmc Samples\n",
        "          then:\n",
        "            - '--mcmc_samples'\n",
        "            - {inputValue: Mcmc Samples}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Interval Width\n",
        "          then:\n",
        "            - '--interval_width'\n",
        "            - {inputValue: Interval Width}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Uncertainty Samples\n",
        "          then:\n",
        "            - '--uncertainty_samples'\n",
        "            - {inputValue: Uncertainty Samples}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Stan Backend\n",
        "          then:\n",
        "            - '--stan_backend'\n",
        "            - {inputValue: Stan Backend}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Cross Validation Horizon\n",
        "          then:\n",
        "            - '--cross_validation_horizon'\n",
        "            - {inputValue: Cross Validation Horizon}\n",
        "      - '--model'\n",
        "      - {outputPath: Model}\n",
        "      - '--cross_validation_results'\n",
        "      - {outputPath: Cross Validation Results}\n",
        "\"\"\")\n",
        "\n",
        "_ProphetPredictOp = load_component_from_text(\"\"\"\n",
        "name: Facebook Prophet Forecast\n",
        "description: Perform a time-series forecast using a fitted Facebook Prophet Model\n",
        "\n",
        "inputs:\n",
        "  - {name: Model, type: Model}\n",
        "  - {name: Future Data Source, type: Artifact, optional: True}\n",
        "  - {name: Periods, type: Integer, optional: True}\n",
        "outputs:\n",
        "  - {name: Prediction, type: Dataset}\n",
        "\n",
        "implementation:\n",
        "  container:\n",
        "    image: us-docker.pkg.dev/vertex-ai/prophet/prophet\n",
        "    command: [\n",
        "      python3,\n",
        "      /pipelines/component/src/prediction.py\n",
        "      ]\n",
        "    args:\n",
        "      - '--model'\n",
        "      - {inputPath: Model}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Future Data Source\n",
        "          then:\n",
        "            - '--future_data_source'\n",
        "            - {inputPath: Future Data Source}\n",
        "      - if:\n",
        "          cond:\n",
        "            isPresent: Periods\n",
        "          then:\n",
        "            - '--periods'\n",
        "            - {inputValue: Periods}\n",
        "      - '--prediction'\n",
        "      - {outputPath: Prediction}\n",
        "\"\"\")\n",
        "\n",
        "# Convert compile-time list arguments to JSON strings\n",
        "def SerializeListArgs(args: tuple, kwargs: dict):\n",
        "  processed_args = [json.dumps(arg) if type(arg) is list else arg for arg in args]\n",
        "  for key, value in kwargs.items():\n",
        "    if type(value) is list:\n",
        "      kwargs[key] = json.dumps(value)\n",
        "  return tuple(processed_args), kwargs\n",
        "\n",
        "\n",
        "def FitProphetModelOp(data_source, *args, **kwargs):\n",
        "  args, kwargs = SerializeListArgs(args, kwargs)\n",
        "  if type(data_source) is str:\n",
        "    data_source = dsl.importer(artifact_uri=data_source, artifact_class=Dataset).output\n",
        "  return _FitProphetModelOp(data_source, *args, **kwargs)\n",
        "\n",
        "def ProphetPredictOp(*args, **kwargs):\n",
        "  args, kwargs = SerializeListArgs(args, kwargs)\n",
        "  if 'future_data_source' in kwargs and type(kwargs['future_data_source']) is str:\n",
        "    kwargs['future_data_source'] = dsl.importer(artifact_uri=kwargs['future_data_source'], artifact_class=Dataset).output\n",
        "  return _ProphetPredictOp(*args, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O7dAiCYy38D"
      },
      "source": [
        "Helper function to compile and run a pipeline on Vertex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEDuLzadzErJ"
      },
      "source": [
        "# Helper function to compile and run a pipeline on Vertex managed pipelines\n",
        "def run_pipeline(pipeline,\n",
        "                 enable_caching=True):\n",
        "  compiler.Compiler().compile(\n",
        "      pipeline_func=pipeline,\n",
        "      package_path='{}.json'.format(pipeline.__name__))\n",
        "\n",
        "  api_client = AIPlatformClient(\n",
        "      project_id='cloud-automl-tables',\n",
        "      region='us-central1',\n",
        "  )\n",
        "\n",
        "  return api_client.create_run_from_job_spec(\n",
        "      job_spec_path='{}.json'.format(pipeline.__name__),\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      enable_caching=enable_caching)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3-IIoPzHye"
      },
      "source": [
        "# Pipeline Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbSRgAM8zL08"
      },
      "source": [
        "@component(packages_to_install=['google-cloud-bigquery', 'pandas', 'pyarrow'])\n",
        "def download_data_op(products: List[str], project: str, output: OutputPath('CSV')):\n",
        "  \"\"\"Download the historical data from BigQuery.\"\"\"\n",
        "  from google.cloud import bigquery\n",
        "  # Sum the number of bottles sold of each product for each day between Jan. 1st\n",
        "  # 2012 to Jan. 1st 2017\n",
        "  query = \"\"\"\n",
        "  SELECT date, LOWER(category_name) as category_name,\n",
        "  SUM(bottles_sold) as bottles_sold\n",
        "  FROM `bigquery-public-data.iowa_liquor_sales.sales`\n",
        "  WHERE LOWER(category_name) IN ( \"{}\" )\n",
        "  AND date BETWEEN '2012-01-01' AND '2017-01-01'\n",
        "  GROUP BY date, category_name\n",
        "  ORDER BY date\"\"\".format('\", \"'.join(products))\n",
        "  bigquery.Client(project=project).query(query).to_dataframe().to_csv(output)\n",
        "\n",
        "\n",
        "@component(packages_to_install=['pandas'])\n",
        "def rename_and_filter_op(data: InputPath('CSV'), output: OutputPath()):\n",
        "  \"\"\"Rename the timestamp and target columns to 'ds' and 'y' respectively.\n",
        "  Additionally, drop columns not needed for analysis.\n",
        "  \"\"\"\n",
        "  import pandas as pd\n",
        "  pd.read_csv(data).rename(columns={\n",
        "      'date': 'ds',\n",
        "      'bottles_sold': 'y',\n",
        "      'category_name': 'product'\n",
        "  })[['ds', 'y', 'product']].to_csv(output)\n",
        "\n",
        "\n",
        "@component(packages_to_install=['pandas'])\n",
        "def select_by_product_op(\n",
        "    data: InputPath('CSV'), product: str, output: OutputPath()):\n",
        "  \"\"\"Drop rows that dont represent the sale of the specified product.\n",
        "  Additionally, drop the column that identifies the product.\"\"\"\n",
        "  import pandas as pd\n",
        "  df = pd.read_csv(data)\n",
        "  df.loc[df['product'].str.lower() == product].drop(\n",
        "      columns=['product']).to_csv(output)\n",
        "\n",
        "@component(base_image='google/cloud-sdk')\n",
        "def upload_model_to_gcs_op(model_file: InputPath(), model_artifact_dir: str,\n",
        "                           model_display_name: str):\n",
        "  # Remove the trailing slash on the model artifact dir if there is one\n",
        "  if model_artifact_dir.endswith('/'):\n",
        "    model_artifact_dir = model_artifact_dir[:-1]\n",
        "  dest_uri = 'gs://{}/{}.model.json'.format(\n",
        "      model_artifact_dir, model_display_name)\n",
        "  import os\n",
        "  os.system('gsutil cp {} {}'.format(model_file, dest_uri))\n",
        "\n",
        "@component(base_image='google/cloud-sdk')\n",
        "def create_batch_request_file_op(categories: List[str], periods: int,\n",
        "                                       gcs_dest: str):\n",
        "  import tempfile\n",
        "  import os\n",
        "  instance_format = '{{\"time_series\":\"{}\", \"periods\":{}}}'\n",
        "  instances = [\n",
        "      instance_format.format(product, periods) for product in categories]\n",
        "  with tempfile.NamedTemporaryFile('w') as f:\n",
        "    f.writelines(instances)\n",
        "    os.system('gsutil cp {} {}'.format(f.name, gcs_dest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBkbOPUsz_ag"
      },
      "source": [
        "# Predicting the sale of Irish whiskies\n",
        "The following pipeline demonstrates how to train and make predictions for a single time-series using the `FitProphetModelOp` and `ProphetPredictOp` components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQRi7ipl0CVy"
      },
      "source": [
        "@dsl.pipeline(name='iowa-irish-whisky-sales-forecast')\n",
        "def irish_whisky_sales_forecast():\n",
        "  download_task = download_data_op('[\"irish whiskies\"]', GCP_PROJECT_ID)\n",
        "  rename_and_filter_task = rename_and_filter_op(download_task.output)\n",
        "  fit_model_task = FitProphetModelOp(rename_and_filter_task.output)\n",
        "  predict_task = ProphetPredictOp(fit_model_task.outputs['model'], periods=365)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn-mvGLz0WLC"
      },
      "source": [
        "run = run_pipeline(irish_whisky_sales_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfFhxbyHTyGZ"
      },
      "source": [
        "# Predicting the sale of various whiskies\n",
        "The previous example demonstrated fitting a single Facebook Prophet model to predict the future sales of a single type of whisky. Multiple Prophet models can be trained in parallel using the `ParallelFor` construct. To make predictions against multiple models, they may be imported as a singular Vertex AI model where Vertex Batch Prediction may be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxD3YsBHT0_t"
      },
      "source": [
        "@dsl.pipeline(name='iowa-multiple-whiskies-sales-forecast')\n",
        "def multiple_whiskies_sales_forecast():\n",
        "  categories = [\n",
        "      'blended whiskies',\n",
        "      'canadian whiskies',\n",
        "      'corn whiskies',\n",
        "      'irish whiskies',\n",
        "      'scotch whiskies',\n",
        "      'straight rye whiskies',\n",
        "  ]\n",
        "\n",
        "  download_task = download_data_op(json.dumps(categories), GCP_PROJECT_ID)\n",
        "  rename_and_filter_task = rename_and_filter_op(download_task.output)\n",
        "  loop_task = ParallelFor(categories)\n",
        "  with loop_task as product:\n",
        "    # The following steps will run for each product\n",
        "    select_task = select_by_product_op(rename_and_filter_task.output, product)\n",
        "    fit_model_task = FitProphetModelOp(select_task.output)\n",
        "    gcs_upload_task = upload_model_to_gcs_op(fit_model_task.outputs['model'],\n",
        "                                             GCS_MODEL_ARTIFACT_DIR, product)\n",
        "  model_import_op = VertexModelImportOp(\n",
        "      project=GCP_PROJECT_ID,\n",
        "      display_name=VERTEX_MODEL_DISPLAY_NAME,\n",
        "      serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prophet/prophet',\n",
        "      artifact_uri=GCS_MODEL_ARTIFACT_DIR,\n",
        "      serving_container_predict_route='/predict',\n",
        "      serving_container_health_route='/health',\n",
        "      serving_container_command=['./online_prediction_start.sh'],\n",
        "      serving_container_ports=[8080]).after(loop_task)\n",
        "\n",
        "  create_br_file_task = create_batch_request_file_op(json.dumps(categories),\n",
        "                                                     365,\n",
        "                                                     BATCH_PREDICT_GCS_SOURCE)\n",
        "\n",
        "  ModelBatchPredictOp(\n",
        "      project=GCP_PROJECT_ID,\n",
        "      model=model_import_op.outputs['model'],\n",
        "      job_display_name='Sales forecast',\n",
        "      gcs_source=BATCH_PREDICT_GCS_SOURCE,\n",
        "      gcs_destination_prefix=BATCH_PREDICT_GCS_PREFIX,\n",
        "      machine_type='n1-standard-2').after(create_br_file_task)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZiSnN2GVazP"
      },
      "source": [
        "run = run_pipeline(multiple_whiskies_sales_forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdSJOLu9Vcay"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}